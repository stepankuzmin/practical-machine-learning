---
title: "Using fitness-trackers to predict how well an activity was performed"
author: "Stepan Kuzmin"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify _how much_ of a particular activity they do, but they rarely quantify _how well_ they do it.

__The goal of this project is to predict the manner in which 6 participants did the barbell lifts using data from their accelerometers on the belt, forearm, arm, and dumbell.__

# Input data

This project uses [Human Activity Recognition Weight Lifting Exercises dataset](http://groupware.les.inf.puc-rio.br/har). Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
library(rpart)
library(rattle)
library(randomForest)
```

```{r, message=FALSE, warning=FALSE, cache=TRUE}
data <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),
                 na.strings=c("NA", "#DIV/0!", ""))
data <- tbl_df(data)
dim(data)
```

Remove first 5 columns, that not make any intuitive sense, columns that contains mostly NA's and zero- or near-zero predictors

```{r, message=FALSE, warning=FALSE, cache=TRUE}
newData <- data %>% select(-c((1:5), which(colMeans(is.na(.)) > 0.95), nearZeroVar(data)))
```

Select numeric columns

```{r}
numericData <- newData %>% select(which(sapply(., is.numeric)))
```

Impute missing values using `knnImpute`

```{r, message=FALSE, warning=FALSE, cache=TRUE}
preprocessedData <- preProcess(numericData, method=c('knnImpute', 'center', 'scale'))
numericData <- predict(preprocessedData, numericData)
processedData <- full_join(newData, numericData)
```

Remove highly correlated variables and all observations with missing predicting variable

```{r, message=FALSE, warning=FALSE, cache=TRUE}
cleanData <- processedData %>% select(-findCorrelation(cor(numericData))) %>% filter(!is.na(classe))
dim(cleanData)
sum(is.na(cleanData$classe))
```

Create cross validation set

```{r}
set.seed(123456)
inTrain <- createDataPartition(y=cleanData$classe, p=0.75, list=F)
training <- cleanData[inTrain, ]
testing <- cleanData[-inTrain, ]
dim(training)
```

# Algorithm

Use a random forest model, for its accuracy and acceptable performance. Fit the model on `training` dataset, and instruct the `train` function to use 3-fold cross-validation to select optimal tuning parameters for the model.

```{r, message=FALSE, warning=FALSE, cache=TRUE}
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
modFit <- train(classe ~ ., data=training, allowParallel=T, trControl=fitControl, method="rf")
```

```{r}
modFit
confusionMatrix(testing$classe, predict(modFit, testing))
```

The overall accuracy for random forest model is __0.998__ what is pretty accurate.

Use classification tree model for its interpretability.

```{r, message=FALSE, warning=FALSE, cache=TRUE}
modFit2 <- train(classe ~ ., data=training, method="rpart")
```

```{r}
modFit2
confusionMatrix(testing$classe, predict(modFit2, testing))
```

The overall accuracy for classification tree model is __0.527__ what is less accurate than random forest model.

```{r}
fancyRpartPlot(modFit2$finalModel)
```

# Cross validation

```{r}
plot(modFit)
```

The cross validation graph shows that the model with 25 predictors is selected by the best accuracy.

```{r}
plot(modFit$finalModel)
```

The final model plot tells that the overall error converge at around 100 trees, so it is possible to speed up our algo by tuning the number of trees.

```{r}
plot(varImp(modFit), top = 10)
```

Top ten important variables

# Evaluation

Predict testing dataset using random forest model

```{r, message=FALSE, warning=FALSE, cache=TRUE}
testingData <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),
                        na.strings=c("NA", "#DIV/0!", ""))
testingData$classe <- NA
cleanTestingData <- testingData %>% select(match(colnames(cleanData), names(.)))
```

```{r}
print(predict(modFit, newdata=cleanTestingData))
```
